{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helps to find the pyspark path\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sparknlp\n",
    "spark = sparknlp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610902e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483db46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sql functions\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file into dataframe\n",
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690846c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read categories description file\n",
    "cat_desc_df=spark.read.option(\"header\",True).csv('Category_Descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_desc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec09f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create temporary view for meta data\n",
    "df.createOrReplaceTempView(\"arxiv_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select required cols from the above temp view\n",
    "df_sub = spark.sql(\"SELECT id,title,abstract,categories,authors FROM arxiv_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the year from versions column; if a doc has multiple versions then fetch max versions year\n",
    "latest_version = df.select('versions').rdd.map(lambda x: int(x[-1][-1][0][-17:-13])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_version[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65300f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pyspark df for year to merge with main df i.e. df_sub\n",
    "lv_df = spark.createDataFrame([(i,) for i in latest_version], ['Year'])\n",
    "\n",
    "#joining df_sub and lv_df dataframes\n",
    "df_sub = df_sub.withColumn(\"row_id\",\n",
    "                           row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "lv_df = lv_df.withColumn(\"row_id\",\n",
    "                         row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "transformed_df = df_sub.join(lv_df, df_sub.row_id == lv_df.row_id).drop(\"row_id\")\n",
    "\n",
    "#left join with transformed_df, all the cats are not in category_desc file\n",
    "transformed_df=transformed_df.join(cat_desc_df,transformed_df.categories == cat_desc_df.categories,\n",
    "                    \"left\").select(transformed_df['*'], cat_desc_df.cat_desc)\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_id_df=transformed_df.select(['id','authors','Year','categories'])\n",
    "\n",
    "from pyspark.sql.functions import split, explode\n",
    "auth_id_df=auth_id_df.withColumn('authors',explode(split('authors',',|\\n| and ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e54840",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_id_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b96413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create temporary view on top the transormed data\n",
    "transformed_df.createOrReplaceTempView(\"transformed_df\")\n",
    "\n",
    "auth_id_df.createOrReplaceTempView(\"auth_id_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676be779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "print('Total Number of Research Papers:',transformed_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e91d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yearly trend\n",
    "rp_year_df=spark.sql('''\n",
    "                        SELECT Year\n",
    "                            ,count(DISTINCT id) as CNT\n",
    "                        FROM transformed_df \n",
    "                        WHERE Year<2022\n",
    "                        GROUP BY 1\n",
    "                        ORDER BY CNT ASC\n",
    "                        \n",
    "                        ''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yearly trend\n",
    "fig_violationtrend = px.line(rp_year_df, x=\"Year\", \n",
    "                             y='CNT', \n",
    "                             title='Yearly Trend of Research Publications')\n",
    "\n",
    "fig_violationtrend.update_layout(yaxis_title=\"#of Research Papers Published\")\n",
    "fig_violationtrend.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2528c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "rp_cat_year_df=spark.sql('''\n",
    "                        SELECT TEMP_B.* FROM(\n",
    "                        SELECT TEMP_A.*,ROW_NUMBER() OVER(PARTITION BY TEMP_A.Year \n",
    "                                            ORDER BY TEMP_A.CNT DESC) as row_n\n",
    "                        FROM\n",
    "                        (\n",
    "                        SELECT categories,Year,count(DISTINCT id) as CNT\n",
    "                        FROM transformed_df \n",
    "                        WHERE Year>2016 AND Year<2022\n",
    "                        GROUP BY 1,2) TEMP_A\n",
    "                        )TEMP_B WHERE TEMP_B.row_n<=3\n",
    "                        \n",
    "                        ''').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d34775",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_cat_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42276fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_rp_cat_year_df = px.bar(rp_cat_year_df,\n",
    "             y='CNT',\n",
    "             x=\"Year\",\n",
    "            color='categories',\n",
    "            text_auto=True\n",
    "                 )\n",
    "\n",
    "fig_rp_cat_year_df.update_layout(yaxis_title=\"#of Research Papers Published\",\n",
    "                                 title='Research Papers of Top 3 Categories Year Wise (last 5 years)')\n",
    "fig_rp_cat_year_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4933d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Number of Distinct Authors:',\n",
    "     spark.sql(\"SELECT count(DISTINCT trim(authors)) FROM auth_id_df\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07160859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8726a09",
   "metadata": {},
   "source": [
    "<b>Pipeline for Spark NLP operations</b> (https://nlp.johnsnowlabs.com/docs/en/annotators)\n",
    "\n",
    "•\t<b>Document Assembler:</b> Prepares data into a format that is processable by Spark NLP (more in above link)\n",
    "\n",
    "•\t<b>Sentence Detector:</b> Detects sentence boundaries using any provided approach.\n",
    "\n",
    "•\t<b>Tokenizer:</b> Tokenizes raw text into word pieces, tokens\n",
    "\n",
    "•\t<b>Normalizer:</b> Removes all dirty characters from text following a regex pattern and transforms words based on a provided   dictionary.\n",
    "\n",
    "•\t<b>Lemmatizer:</b> Finds lemmas out of words with the objective of returning a base dictionary word.\n",
    "\n",
    "•\t<b>Stopwords Cleaner:</b> This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences.\n",
    "\n",
    "•\t<b>Finisher:</b> Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e004f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30851886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotate data to document type, which will be used further\n",
    "doc_assembler = DocumentAssembler().setInputCol('abstract').setOutputCol('doc')\n",
    "\n",
    "sentence_detector = SentenceDetector().setInputCols(['doc']).setOutputCol('sentences')\n",
    "\n",
    "#each word in the doc/sentence is tokenized\n",
    "tokenization = Tokenizer().setInputCols(['sentences']).setOutputCol('tokenizer_out')\n",
    "\n",
    "#normalization, converts to lower case, removes special characters\n",
    "normalization = Normalizer().setInputCols(['tokenizer_out']).setOutputCol('normalized_out') \\\n",
    "     .setLowercase(True).setCleanupPatterns([\"[^\\w\\d\\s]\"])\n",
    "\n",
    "#lemmatization, converts to root word\n",
    "lemmatization = Lemmatizer().setInputCols([\"normalized_out\"]).setOutputCol(\"lemma_out\") \\\n",
    "    .setDictionary(\"AntBNC_lemmas_ver_001.txt.txt\",value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
    "\n",
    "#removes stopwords like is, the etc.\n",
    "rm_stopwords= StopWordsCleaner().setInputCols(['lemma_out']).setOutputCol('rm_stopwords_out') \\\n",
    "     .setCaseSensitive(False).setStopWords(eng_stopwords)\n",
    "\n",
    "\n",
    "#readable output\n",
    "readable_out_finisher = Finisher().setInputCols(['rm_stopwords_out']) \\\n",
    "     .setOutputCols('finshed_lemma').setCleanAnnotations(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the pipeline\n",
    "nlpPipeline = Pipeline(stages=[doc_assembler\n",
    "                               ,sentence_detector\n",
    "                               ,tokenization\n",
    "                               ,normalization\n",
    "                               ,lemmatization\n",
    "                               ,rm_stopwords\n",
    "                               ,readable_out_finisher\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578d147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006e368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the data for TF-IDF operation as this operation is taking time, \n",
    "#filter on categories only starts with 'cs' and year>2016\n",
    "\n",
    "trans_sub_df = spark.sql('''\n",
    "                            SELECT * \n",
    "                            FROM transformed_df \n",
    "                            where categories LIKE 'cs.CV' \n",
    "                                and Year>2016 AND Year<2022\n",
    "                            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_sub_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_sub_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_df=trans_sub_df.select('abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_nlp_trans = nlpPipeline.fit(abstract_df).transform(abstract_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb267ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lemma_df = abstract_nlp_trans.select('finshed_lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(abstract_lemma_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lemma_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bfe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb929a20",
   "metadata": {},
   "source": [
    "<b>Term Frequency and IDF</b> (https://spark.apache.org/docs/latest/ml-features.html#tf-idf)\n",
    "\n",
    "<b>Term Frequency:</b> \n",
    "\n",
    "•\tWe find frequency of each word that is appearing in the text. Output would be term frequency vector.\n",
    "\n",
    "•\tWe can achieve the above by CountVectorizer(CV) or HashingTF. Obviously, there are advantages and disadvantages for both methods. \n",
    "\n",
    "•\t<b>HasingTF:</b>\n",
    "\n",
    "    • It is faster but suffers collision effect i.e., same hasing index can be appeared to multiple terms.\n",
    "    • Because of collision effect results might not be that accurate when compared to CV method \n",
    "    • It is hard to understand in the sense that TF vector hard to trace back to the original words in a doc\n",
    "\n",
    "•\t<b>CountVectorizer:</b>\n",
    "\n",
    "    • Gives more accurate results\n",
    "    • Easy understand back track meaning the TF vectors can be traced back to the exact word\n",
    "    • Suffers performance issues on larger datasets\n",
    "\n",
    "\n",
    "<b>IDF (Inverse Document Frequency):</b> \n",
    "\n",
    "•\tIt takes feature vectors created from above TF methods and downs the weight of features which appear frequently in the collection documents/texts/corpus. \n",
    "\n",
    "•\tOutput from this can be used in ML algorithms like classification, topic modeling etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70111b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74156cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#term frequency computation\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "#minDF = 2 means \"ignore terms that appear in less than 2 documents\"\n",
    "#maxDF = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "\n",
    "countvectorizer = CountVectorizer().setInputCol(\"finshed_lemma\").setOutputCol(\"features_cv\") \\\n",
    "                 .setMinDF(10).setMaxDF(0.75)\n",
    "model_cv = countvectorizer.fit(abstract_lemma_df)\n",
    "model_cv_out = model_cv.transform(abstract_lemma_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF computation\n",
    "from pyspark.ml.feature import IDF\n",
    "model_idf = IDF(inputCol=\"features_cv\", outputCol=\"features_idf\").fit(model_cv_out)\n",
    "#.setInputCol(\"features_cv\").setOutputCol(\"features_idf\")\n",
    "\n",
    "#model_IDF_fit = model_idf.fit(model_cv_out)\n",
    "model_IDF_out = model_idf.transform(model_cv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_IDF_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0294620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c43a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b481142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab7b60e",
   "metadata": {},
   "source": [
    "<b>NLU (Natural Language Understanding) </b>\n",
    "(Ref: https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
    "\n",
    "We try to understand the document by analysing its topics.\n",
    "\n",
    "<b>Topic modeling: </b>\n",
    "\n",
    "    •From the corpus of documents/texts, we try to recognize and extract topics across the collection of documents\n",
    "    •Each document can have mixture of topics and each topic consists of collection of words\n",
    "\n",
    "<b>There many ways to extract the latent/hidden topics from collection of documents. Below are some of them</b>\n",
    "\n",
    "    •LDA (Latent Dirichlet Allocation)\n",
    "    •LSA (Latent Semantic Analysis)\n",
    "    •pLSA (Probabilistic Semantic Analysis)\n",
    "    •BERT Topic modelling\n",
    "\n",
    "<b>LDA (Latent Dirichlet Allocation): </b> (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html)\n",
    "\n",
    "    •Finds document-topic and word-topic distributions using Dirichlet priors.\n",
    "    •Simply put distributions over distributions, meaning given a particular type of distribution what are probability distributions, we expect to see.\n",
    "    •LDA model is given collection of docs as input where each doc is specified as vector of length VocabSize, each entry is count for corresponding word in the document.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modeling using LDA(Latent Dirichlet Allocation)\n",
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(k=20,maxIter=100,featuresCol='features_idf')\n",
    "model_LDA = lda.fit(model_IDF_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_LDA = model_LDA.transform(model_IDF_out)\n",
    "#transformed_LDA.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be79293",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_LDA.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87514508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fetch words from vocabulary\n",
    "import pyspark.sql.types as T\n",
    "docs_word_list = model_cv.vocabulary\n",
    "def fetch_words(token_list):\n",
    "    return [docs_word_list[i] for i in token_list]\n",
    "fun_to_words = F.udf(fetch_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "num_top_words = 50\n",
    "topics_LDA = model_LDA.describeTopics(num_top_words).withColumn('words_in_topic', fun_to_words(F.col('termIndices')))\n",
    "#topics_LDA.select('topic', 'words_in_topic').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ab4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f120b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_df=trans_sub_df.select('id')\n",
    "\n",
    "#joining df_sub and lv_df dataframes\n",
    "docid_df = docid_df.withColumn(\"row_id\",\n",
    "                           row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "model_IDF_out = model_IDF_out.withColumn(\"row_id\",\n",
    "                         row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "Final_model_IDF_out = model_IDF_out.join(docid_df, model_IDF_out.row_id == docid_df.row_id).drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7ee15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6377fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_LDA = topics_LDA.withColumn(\"row_id\",\n",
    "                         row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "Final_topics_LDA = topics_LDA.join(docid_df, topics_LDA.row_id == docid_df.row_id).drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca874a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_LDA = transformed_LDA.withColumn(\"row_id\",\n",
    "                         row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "Final_transformed_LDA = transformed_LDA.join(docid_df, transformed_LDA.row_id == docid_df.row_id).drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trans_sub_df DataTypes:\\n',trans_sub_df.dtypes,'\\n')\n",
    "\n",
    "print('Final_model_IDF_out DataTypes:\\n',Final_model_IDF_out.dtypes,'\\n')\n",
    "\n",
    "print('Final_topics_LDA DataTypes:\\n',Final_topics_LDA.dtypes,'\\n')\n",
    "\n",
    "print('Final_transformed_LDA DataTypes:\\n',Final_transformed_LDA.dtypes,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681fd67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f63a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trans_sub_df.write.mode('overwrite').parquet(\"trans_sub_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73265aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_model_IDF_out.write.mode('overwrite').parquet(\"trans_sub_df_tfidf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_topics_LDA.write.mode('overwrite').parquet(\"topics_LDA.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_transformed_LDA.write.mode('overwrite').parquet(\"transformed_LDA_TopicDist.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_2=pd.read_parquet(\"trans_sub_df_tfidf.parquet\",engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7f731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbe796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd51ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
